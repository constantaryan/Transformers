# Transformers
#Transformers from Scratch - Reimplementation of "Attention Is All You Need"

This repository contains a complete, from-scratch reimplementation of the legendary paper **"Attention Is All You Need"** by Vaswani et al., using PyTorch and a character-level toy dataset. The goal is to deeply understand the inner workings of the Transformer architecture, focusing on how attention mechanisms power modern NLP.

---

## ðŸ“Œ Project Highlights

-  Inspired by the original [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.
- Fully built from scratch â€” **no use of high-level APIs like `nn.Transformer`**.
-  Implements key concepts:
  - Positional Encoding
  - Scaled Dot-Product Attention
  - Multi-Head Attention
  - Encoder-Decoder Structure
  - Layer Normalization and Residuals
- Trained on a simple **character-level toy dataset** for easy visualization and debugging.

---

